@def title = "Counterfactuals: Causal Inference vs Optimization"
@def published = "17 October 2025"
@def tags = ["causal-inference", "optimization"]

# Counterfactuals: Causal Inference vs Optimization

## Mathematical Formulations

### Causal Inference Counterfactuals

**Framework:** Potential Outcomes or Structural Causal Models (SCMs)

**Notation (Potential Outcomes):**
- $Y_i(1)$ = outcome for unit $i$ under treatment
- $Y_i(0)$ = outcome for unit $i$ under control
- Individual Treatment Effect: $\tau_i = Y_i(1) - Y_i(0)$

**Notation (SCM/Pearl):**
- $Y_{X=x}$ or $Y | do(X=x)$ = value of $Y$ when we intervene to set $X=x$
- Uses structural equations: $Y = f(X, U)$ where $U$ are exogenous variables
- Counterfactual: $Y_{X=x'}(u) = f(x', u)$ for observed $u$

> **ðŸ“ Side Note: Structural Equations & Exogenous Variables**
>
> **Structural Equations** describe the causal mechanism generating each variable:
> - Format: `Y = f(Parents(Y), U_Y)` 
> - The "=" means "is generated by", not just "is correlated with"
> - They tell you HOW to compute outcomes, not just statistical relationships
> 
> Example:
> ```
> X = U_X                  (X has no causes, just noise)
> Y = 2X + U_Y            (Y is caused by X)
> Z = 3Y - X + U_Z        (Z depends on both Y and X)
> ```
>
> **Exogenous Variables (U)** are "external to the system":
> - Not caused by anything else in your model
> - Represent all unmeasured factors (talent, luck, genes, etc.)
> - **Stay fixed in counterfactual reasoning** - this is crucial!
>
> **Why U matters for counterfactuals:**
> When you observe someone, you implicitly observe their U values. 
> In a counterfactual, you keep THEIR specific U values fixed while 
> changing the intervention.
>
> Example: "You scored 80 with 10 hours of study. What if you studied 15?"
> - Your talent (U_talent) stays the same
> - Only study hours change
> - This gives YOUR counterfactual outcome, not the population average
>
> This is how Pearl's framework handles **individual-level** counterfactuals,
> not just population-level effects!

**Key Properties:**
- Based on causal graphical models
- Requires identifiability conditions (e.g., no unmeasured confounding)
- Involves three steps: abduction (infer $U$ from observations), action (intervene), prediction
- Fundamentally about **causal mechanisms**

**Example:**
```
Patient took drug A, recovered in 5 days
Counterfactual: Would they recover in 3 days with drug B?
Requires: Causal model of how drugs affect recovery
```

---

### Optimization Counterfactuals

**Framework:** Constrained optimization problem

**Standard Formulation:**
$$\min_{x'} d(x, x') \quad \text{subject to} \quad f(x') \neq f(x)$$

or more specifically:

$$\min_{x'} \|x - x'\|_p + \lambda \cdot \text{cost}(x') \quad \text{s.t.} \quad h(x') = y_{\text{target}}$$

where:
- $x$ = original instance
- $x'$ = counterfactual instance
- $d(x, x')$ = distance metric (often L1, L2, or Mahalanobis)
- $f$ or $h$ = prediction model
- Optional constraints: feasibility, actionability, sparsity

**Key Properties:**
- Treats model as black-box or uses gradients
- Focus on **proximity** and **actionability**
- May include cost functions for realistic changes
- Does not necessarily respect causal structure

**Example:**
```
Loan rejected with income=$50k, debt=$30k
Counterfactual: income=$55k, debt=$28k â†’ approved
Found by: minimizing distance in feature space
```

---

## Similarities

Despite their mathematical differences, these two approaches share important conceptual ground:

### 1. **Both Ask "What If?" Questions**
- Causal inference: "What if we had given treatment instead of control?"
- Optimization: "What if this feature value were different?"
- Both explore **alternative scenarios** from an observed baseline

### 2. **Both Involve Minimality/Proximity**
- **Causal counterfactuals:** In Pearl's framework, exogenous variables U stay fixed - you change as little as possible about the "world state"
- **Optimization counterfactuals:** Explicitly minimize distance $d(x, x')$ - you change as little as possible about the input
- Both prefer **minimal interventions** over arbitrary changes

### 3. **Both Distinguish Intervention from Observation**
- **Causal inference:** $P(Y|do(X=x)) \neq P(Y|X=x)$ - intervention â‰  conditioning
- **Optimization (when done right):** Changing income directly â‰  observing someone with that income
- Both recognize that **making something true** is different from **observing it to be true**

### 4. **Both Face Feasibility Constraints**
- **Causal counterfactuals:** Can only intervene on certain variables (not age, not past events)
- **Optimization counterfactuals:** Actionability constraints (can change study hours, not inherent talent)
- Both need to respect **what can actually be changed**

### 5. **Mathematical Connection: SCM as Constraints**

An SCM can be viewed as defining a **feasible set** for optimization:

```
Causal SCM:
  Y = f_Y(X, U_Y)
  Z = f_Z(Y, X, U_Z)

Optimization perspective:
  minimize ||x' - x||
  subject to: y' = f_Y(x', u_Y)  [SCM as constraint]
              z' = f_Z(y', x', u_Z)  [SCM as constraint]
              z' âˆˆ desired_class
```

The structural equations become **constraints** that valid counterfactuals must satisfy!

### 6. **Both Can Be Framed as Inverse Problems**
- **Causal inference:** Given outcome Y, infer what intervention X would produce it
- **Optimization:** Given desired outcome y_target, find input x' that achieves it
- Both involve **working backwards** from outcomes to causes/inputs

---

## Key Differences

| Aspect | Causal Inference | Optimization |
|--------|-----------------|--------------|
| **Goal** | Understand causal effects | Find actionable changes |
| **Mathematical Object** | Potential outcome under intervention | Nearby point with different prediction |
| **Requires** | Causal graph/assumptions | Prediction model + distance metric |
| **Validity** | Must respect causal mechanisms | May violate causality |
| **Observability** | Fundamentally unobservable (identification problem) | Computable given model |
| **Constraints** | Exogenous variables remain fixed | Often only model constraints |
| **Uniqueness** | Unique given causal model & data | Many possible solutions |

---

## The Bridge: Causal Counterfactual Explanations

Modern research combines both approaches:

**Causally-Constrained Optimization:**
$$\min_{x'} d(x, x') \quad \text{s.t.} \begin{cases} f(x') = y_{\text{target}} \\ x' \text{ respects causal graph } G \\ \text{only actionable variables changed} \end{cases}$$

This means:
- Use optimization framework for computational tractability
- Add causal constraints: only change variables that can be intervened on
- Respect causal ordering: don't change descendants before ancestors
- Maintain consistency with structural equations

**Example:**
```
Bad optimization counterfactual: "If you were younger, loan approved"
  â†’ Age is not actionable!

Good causal counterfactual: "If you increase income by $5k (feasible 
  action), and this reduces your debt-to-income ratio (causal effect), 
  then loan would be approved"
```

---

## Summary

- **Causal inference counterfactuals** are about **what would happen** under different causal interventions
- **Optimization counterfactuals** are about **what to change** to get different outcomes
- They are mathematically different: one defines a causal quantity, the other solves a proximity problem
- Best practice: use optimization methods constrained by causal knowledge