<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>Counterfactuals: Causal Inference vs Optimization</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">Mathematics</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="counterfactuals_causal_inference_vs_optimization"><a href="#counterfactuals_causal_inference_vs_optimization" class="header-anchor">Counterfactuals: Causal Inference vs Optimization</a></h1>
<h2 id="mathematical_formulations"><a href="#mathematical_formulations" class="header-anchor">Mathematical Formulations</a></h2>
<h3 id="causal_inference_counterfactuals"><a href="#causal_inference_counterfactuals" class="header-anchor">Causal Inference Counterfactuals</a></h3>
<p><strong>Framework:</strong> Potential Outcomes or Structural Causal Models &#40;SCMs&#41;</p>
<p><strong>Notation &#40;Potential Outcomes&#41;:</strong></p>
<ul>
<li><p>\(Y_i(1)\) &#61; outcome for unit \(i\) under treatment</p>
</li>
<li><p>\(Y_i(0)\) &#61; outcome for unit \(i\) under control</p>
</li>
<li><p>Individual Treatment Effect: \(\tau_i = Y_i(1) - Y_i(0)\)</p>
</li>
</ul>
<p><strong>Notation &#40;SCM/Pearl&#41;:</strong></p>
<ul>
<li><p>\(Y_{X=x}\) or \(Y | do(X=x)\) &#61; value of \(Y\) when we intervene to set \(X=x\)</p>
</li>
<li><p>Uses structural equations: \(Y = f(X, U)\) where \(U\) are exogenous variables</p>
</li>
<li><p>Counterfactual: \(Y_{X=x'}(u) = f(x', u)\) for observed \(u\)</p>
</li>
</ul>
<blockquote>
<p><strong>üìù Side Note: Structural Equations &amp; Exogenous Variables</strong></p>
<p><strong>Structural Equations</strong> describe the causal mechanism generating each variable:</p>
<ul>
<li><p>Format: <code>Y &#61; f&#40;Parents&#40;Y&#41;, U_Y&#41;</code> </p>
</li>
<li><p>The &quot;&#61;&quot; means &quot;is generated by&quot;, not just &quot;is correlated with&quot;</p>
</li>
<li><p>They tell you HOW to compute outcomes, not just statistical relationships</p>
</li>
</ul>
<p>Example:</p>
</blockquote>
<pre><code class="language-julia">&gt; X &#61; U_X                  &#40;X has no causes, just noise&#41;
&gt; Y &#61; 2X &#43; U_Y            &#40;Y is caused by X&#41;
&gt; Z &#61; 3Y - X &#43; U_Z        &#40;Z depends on both Y and X&#41;
&gt;</code></pre>
<blockquote>
<p><strong>Exogenous Variables &#40;U&#41;</strong> are &quot;external to the system&quot;:</p>
<ul>
<li><p>Not caused by anything else in your model</p>
</li>
<li><p>Represent all unmeasured factors &#40;talent, luck, genes, etc.&#41;</p>
</li>
<li><p><strong>Stay fixed in counterfactual reasoning</strong> - this is crucial&#33;</p>
</li>
</ul>
<p><strong>Why U matters for counterfactuals:</strong> When you observe someone, you implicitly observe their U values.  In a counterfactual, you keep THEIR specific U values fixed while  changing the intervention.</p>
<p>Example: &quot;You scored 80 with 10 hours of study. What if you studied 15?&quot;</p>
<ul>
<li><p>Your talent &#40;U_talent&#41; stays the same</p>
</li>
<li><p>Only study hours change</p>
</li>
<li><p>This gives YOUR counterfactual outcome, not the population average</p>
</li>
</ul>
<p>This is how Pearl&#39;s framework handles <strong>individual-level</strong> counterfactuals, not just population-level effects&#33;</p>
</blockquote>
<p><strong>Key Properties:</strong></p>
<ul>
<li><p>Based on causal graphical models</p>
</li>
<li><p>Requires identifiability conditions &#40;e.g., no unmeasured confounding&#41;</p>
</li>
<li><p>Involves three steps: abduction &#40;infer \(U\) from observations&#41;, action &#40;intervene&#41;, prediction</p>
</li>
<li><p>Fundamentally about <strong>causal mechanisms</strong></p>
</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-julia">Patient took drug A, recovered in 5 days
Counterfactual: Would they recover in 3 days with drug B?
Requires: Causal model of how drugs affect recovery</code></pre>
<hr />
<h3 id="optimization_counterfactuals"><a href="#optimization_counterfactuals" class="header-anchor">Optimization Counterfactuals</a></h3>
<p><strong>Framework:</strong> Constrained optimization problem</p>
<p><strong>Standard Formulation:</strong></p>
\[\min_{x'} d(x, x') \quad \text{subject to} \quad f(x') \neq f(x)\]
<p>or more specifically:</p>
\[\min_{x'} \|x - x'\|_p + \lambda \cdot \text{cost}(x') \quad \text{s.t.} \quad h(x') = y_{\text{target}}\]
<p>where:</p>
<ul>
<li><p>\(x\) &#61; original instance</p>
</li>
<li><p>\(x'\) &#61; counterfactual instance</p>
</li>
<li><p>\(d(x, x')\) &#61; distance metric &#40;often L1, L2, or Mahalanobis&#41;</p>
</li>
<li><p>\(f\) or \(h\) &#61; prediction model</p>
</li>
<li><p>Optional constraints: feasibility, actionability, sparsity</p>
</li>
</ul>
<p><strong>Key Properties:</strong></p>
<ul>
<li><p>Treats model as black-box or uses gradients</p>
</li>
<li><p>Focus on <strong>proximity</strong> and <strong>actionability</strong></p>
</li>
<li><p>May include cost functions for realistic changes</p>
</li>
<li><p>Does not necessarily respect causal structure</p>
</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-julia">Loan rejected with income&#61;&#36;50k, debt&#61;&#36;30k
Counterfactual: income&#61;&#36;55k, debt&#61;&#36;28k ‚Üí approved
Found by: minimizing distance in feature space</code></pre>
<hr />
<h2 id="similarities"><a href="#similarities" class="header-anchor">Similarities</a></h2>
<p>Despite their mathematical differences, these two approaches share important conceptual ground:</p>
<h3 id="both_ask_what_if_questions"><a href="#both_ask_what_if_questions" class="header-anchor"><ol>
<li><p><strong>Both Ask &quot;What If?&quot; Questions</strong></p>
</li>
</ol>
</a></h3>
<ul>
<li><p>Causal inference: &quot;What if we had given treatment instead of control?&quot;</p>
</li>
<li><p>Optimization: &quot;What if this feature value were different?&quot;</p>
</li>
<li><p>Both explore <strong>alternative scenarios</strong> from an observed baseline</p>
</li>
</ul>
<h3 id="ol_start2_both_involve_minimalityproximity"><a href="#ol_start2_both_involve_minimalityproximity" class="header-anchor"><ol start="2">
<li><p><strong>Both Involve Minimality/Proximity</strong></p>
</li>
</ol>
</a></h3>
<ul>
<li><p><strong>Causal counterfactuals:</strong> In Pearl&#39;s framework, exogenous variables U stay fixed - you change as little as possible about the &quot;world state&quot;</p>
</li>
<li><p><strong>Optimization counterfactuals:</strong> Explicitly minimize distance \(d(x, x')\) - you change as little as possible about the input</p>
</li>
<li><p>Both prefer <strong>minimal interventions</strong> over arbitrary changes</p>
</li>
</ul>
<h3 id="ol_start3_both_distinguish_intervention_from_observation"><a href="#ol_start3_both_distinguish_intervention_from_observation" class="header-anchor"><ol start="3">
<li><p><strong>Both Distinguish Intervention from Observation</strong></p>
</li>
</ol>
</a></h3>
<ul>
<li><p><strong>Causal inference:</strong> \(P(Y|do(X=x)) \neq P(Y|X=x)\) - intervention ‚â† conditioning</p>
</li>
<li><p><strong>Optimization &#40;when done right&#41;:</strong> Changing income directly ‚â† observing someone with that income</p>
</li>
<li><p>Both recognize that <strong>making something true</strong> is different from <strong>observing it to be true</strong></p>
</li>
</ul>
<h3 id="ol_start4_both_face_feasibility_constraints"><a href="#ol_start4_both_face_feasibility_constraints" class="header-anchor"><ol start="4">
<li><p><strong>Both Face Feasibility Constraints</strong></p>
</li>
</ol>
</a></h3>
<ul>
<li><p><strong>Causal counterfactuals:</strong> Can only intervene on certain variables &#40;not age, not past events&#41;</p>
</li>
<li><p><strong>Optimization counterfactuals:</strong> Actionability constraints &#40;can change study hours, not inherent talent&#41;</p>
</li>
<li><p>Both need to respect <strong>what can actually be changed</strong></p>
</li>
</ul>
<h3 id="ol_start5_mathematical_connection_scm_as_constraints"><a href="#ol_start5_mathematical_connection_scm_as_constraints" class="header-anchor"><ol start="5">
<li><p><strong>Mathematical Connection: SCM as Constraints</strong></p>
</li>
</ol>
</a></h3>
<p>An SCM can be viewed as defining a <strong>feasible set</strong> for optimization:</p>
<pre><code class="language-julia">Causal SCM:
  Y &#61; f_Y&#40;X, U_Y&#41;
  Z &#61; f_Z&#40;Y, X, U_Z&#41;

Optimization perspective:
  minimize ||x&#39; - x||
  subject to: y&#39; &#61; f_Y&#40;x&#39;, u_Y&#41;  &#91;SCM as constraint&#93;
              z&#39; &#61; f_Z&#40;y&#39;, x&#39;, u_Z&#41;  &#91;SCM as constraint&#93;
              z&#39; ‚àà desired_class</code></pre>
<p>The structural equations become <strong>constraints</strong> that valid counterfactuals must satisfy&#33;</p>
<h3 id="ol_start6_both_can_be_framed_as_inverse_problems"><a href="#ol_start6_both_can_be_framed_as_inverse_problems" class="header-anchor"><ol start="6">
<li><p><strong>Both Can Be Framed as Inverse Problems</strong></p>
</li>
</ol>
</a></h3>
<ul>
<li><p><strong>Causal inference:</strong> Given outcome Y, infer what intervention X would produce it</p>
</li>
<li><p><strong>Optimization:</strong> Given desired outcome y_target, find input x&#39; that achieves it</p>
</li>
<li><p>Both involve <strong>working backwards</strong> from outcomes to causes/inputs</p>
</li>
</ul>
<hr />
<h2 id="key_differences"><a href="#key_differences" class="header-anchor">Key Differences</a></h2>
<table><tr><th align="right">Aspect</th><th align="right">Causal Inference</th><th align="right">Optimization</th></tr><tr><td align="right"><strong>Goal</strong></td><td align="right">Understand causal effects</td><td align="right">Find actionable changes</td></tr><tr><td align="right"><strong>Mathematical Object</strong></td><td align="right">Potential outcome under intervention</td><td align="right">Nearby point with different prediction</td></tr><tr><td align="right"><strong>Requires</strong></td><td align="right">Causal graph/assumptions</td><td align="right">Prediction model &#43; distance metric</td></tr><tr><td align="right"><strong>Validity</strong></td><td align="right">Must respect causal mechanisms</td><td align="right">May violate causality</td></tr><tr><td align="right"><strong>Observability</strong></td><td align="right">Fundamentally unobservable &#40;identification problem&#41;</td><td align="right">Computable given model</td></tr><tr><td align="right"><strong>Constraints</strong></td><td align="right">Exogenous variables remain fixed</td><td align="right">Often only model constraints</td></tr><tr><td align="right"><strong>Uniqueness</strong></td><td align="right">Unique given causal model &amp; data</td><td align="right">Many possible solutions</td></tr></table>
<hr />
<h2 id="the_bridge_causal_counterfactual_explanations"><a href="#the_bridge_causal_counterfactual_explanations" class="header-anchor">The Bridge: Causal Counterfactual Explanations</a></h2>
<p>Modern research combines both approaches:</p>
<p><strong>Causally-Constrained Optimization:</strong></p>
\[\min_{x'} d(x, x') \quad \text{s.t.} \begin{cases} f(x') = y_{\text{target}} \\ x' \text{ respects causal graph } G \\ \text{only actionable variables changed} \end{cases}\]
<p>This means:</p>
<ul>
<li><p>Use optimization framework for computational tractability</p>
</li>
<li><p>Add causal constraints: only change variables that can be intervened on</p>
</li>
<li><p>Respect causal ordering: don&#39;t change descendants before ancestors</p>
</li>
<li><p>Maintain consistency with structural equations</p>
</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-julia">Bad optimization counterfactual: &quot;If you were younger, loan approved&quot;
  ‚Üí Age is not actionable&#33;

Good causal counterfactual: &quot;If you increase income by &#36;5k &#40;feasible 
  action&#41;, and this reduces your debt-to-income ratio &#40;causal effect&#41;, 
  then loan would be approved&quot;</code></pre>
<hr />
<h2 id="summary"><a href="#summary" class="header-anchor">Summary</a></h2>
<ul>
<li><p><strong>Causal inference counterfactuals</strong> are about <strong>what would happen</strong> under different causal interventions</p>
</li>
<li><p><strong>Optimization counterfactuals</strong> are about <strong>what to change</strong> to get different outcomes</p>
</li>
<li><p>They are mathematically different: one defines a causal quantity, the other solves a proximity problem</p>
</li>
<li><p>Best practice: use optimization methods constrained by causal knowledge</p>
</li>
</ul>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: October 17, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
