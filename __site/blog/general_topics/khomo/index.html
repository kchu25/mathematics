<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>Function Composition and Homogeneity</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">Mathematics</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="function_composition_and_homogeneity"><a href="#function_composition_and_homogeneity" class="header-anchor">Function Composition and Homogeneity</a></h1>
<h2 id="key_insight"><a href="#key_insight" class="header-anchor">Key Insight</a></h2>
<p><strong>A composition of functions is 1-homogeneous if and only if NO function in the chain is 0-homogeneous.</strong></p>
<h2 id="mathematical_definitions"><a href="#mathematical_definitions" class="header-anchor">Mathematical Definitions</a></h2>
<h3 id="k-homogeneous_function"><a href="#k-homogeneous_function" class="header-anchor">k-Homogeneous Function</a></h3>
<p>A function \(f\) is <strong>k-homogeneous</strong> &#40;or <strong>homogeneous of degree k</strong>&#41; if:</p>
\[f(\alpha x) = \alpha^k \cdot f(x)\]
<ul>
<li><p><strong>1-homogeneous</strong>: \(f(\alpha x) = \alpha \cdot f(x)\) &#40;scales linearly&#41;</p>
</li>
<li><p><strong>0-homogeneous</strong>: \(f(\alpha x) = f(x)\) &#40;scale-invariant&#41;</p>
</li>
</ul>
<blockquote>
<p><strong>Note</strong>: This is the standard definition from mathematics. Formally, a function \(f: \mathbb{R}^n \to \mathbb{R}^m\) is homogeneous of degree \(k\) if \(f(tx) = t^k f(x)\) for all \(t > 0\) &#40;or sometimes for all \(t \in \mathbb{R}\) depending on context&#41;. Also called &quot;positive homogeneity&quot; or &quot;Euler homogeneity&quot;. In economics and physics, this concept describes how systems respond to scaling.</p>
</blockquote>
<h2 id="composition_rules"><a href="#composition_rules" class="header-anchor">Composition Rules</a></h2>
<p>For a composition \(h(x) = f(g(x))\):</p>
<h3 id="if_both_are_1-homogeneous"><a href="#if_both_are_1-homogeneous" class="header-anchor">If both are 1-homogeneous:</a></h3>
\[h(\alpha x) = f(g(\alpha x)) = f(\alpha \cdot g(x)) = \alpha \cdot f(g(x)) = \alpha \cdot h(x)\]
<p>‚úÖ <strong>Result: 1-homogeneous</strong></p>
<h3 id="if_g_is_0-homogeneous"><a href="#if_g_is_0-homogeneous" class="header-anchor">If g is 0-homogeneous:</a></h3>
\[h(\alpha x) = f(g(\alpha x)) = f(g(x)) = h(x)\]
<p>‚ùå <strong>Result: 0-homogeneous</strong> &#40;scaling absorbed&#33;&#41;</p>
<h3 id="general_rule"><a href="#general_rule" class="header-anchor">General Rule:</a></h3>
<p>If \(f\) is \(k\)-homogeneous and \(g\) is \(m\)-homogeneous:</p>
\[h(\alpha x) = f(g(\alpha x)) = f(\alpha^m \cdot g(x)) = \alpha^{km} \cdot f(g(x)) = \alpha^{km} \cdot h(x)\]
<p>The composition is <strong>\((km)\)-homogeneous</strong>.</p>
<h2 id="proof_why_all_functions_must_be_1-homogeneous"><a href="#proof_why_all_functions_must_be_1-homogeneous" class="header-anchor">Proof: Why ALL Functions Must Be 1-Homogeneous</a></h2>
<p><strong>Theorem</strong>: For composition \(h = f_n \circ f_{n-1} \circ \cdots \circ f_1\) to be 1-homogeneous, <strong>every</strong> \(f_i\) must be 1-homogeneous.</p>
<p><strong>Proof</strong>:</p>
<p>Suppose \(f_i\) is \(k_i\)-homogeneous for each \(i\). Then:</p>
\[\begin{align}
h(\alpha x) &= f_n(f_{n-1}(\cdots f_1(\alpha x))) \\
&= f_n(f_{n-1}(\cdots (\alpha^{k_1} f_1(x)))) \\
&= f_n(f_{n-1}(\cdots (\alpha^{k_1 k_2} f_2(f_1(x))))) \\
&= f_n(\alpha^{k_1 k_2 \cdots k_{n-1}} f_{n-1}(\cdots f_1(x))) \\
&= \alpha^{k_1 k_2 \cdots k_n} f_n(f_{n-1}(\cdots f_1(x))) \\
&= \alpha^{k_1 k_2 \cdots k_n} h(x)
\end{align}\]
<p>For 1-homogeneity, we need:</p>
\[\alpha^{k_1 k_2 \cdots k_n} = \alpha\]
<p>This must hold for <strong>all</strong> \(\alpha > 0\). </p>
<p>Taking logarithms: \(k_1 k_2 \cdots k_n = 1\)</p>
<p><strong>Key insight</strong>: The only way the product equals 1 <strong>for all possible homogeneities</strong> is if:</p>
\[k_1 = k_2 = \cdots = k_n = 1\]
<p><strong>Why?</strong></p>
<ul>
<li><p>If any \(k_i = 0\), then product &#61; 0 &#40;0-homogeneous composition&#41;</p>
</li>
<li><p>If any \(k_i \neq 1\), you need other \(k_j\) values to compensate, but this only works for specific choices</p>
</li>
<li><p>Only \(k_i = 1\) for all \(i\) works <strong>universally</strong></p>
</li>
</ul>
<p>Therefore, <strong>every function in the composition must be 1-homogeneous</strong>. ‚àé</p>
<h2 id="critical_observation"><a href="#critical_observation" class="header-anchor">Critical Observation</a></h2>
<p><strong>Any 0-homogeneous function acts as a &quot;scaling absorber&quot;:</strong></p>
<ul>
<li><p>Input: \(\alpha x\)</p>
</li>
<li><p>Output: Same as \(f(x)\)</p>
</li>
<li><p>The factor \(\alpha\) is completely eliminated</p>
</li>
</ul>
<p><strong>Once scaling is absorbed, it cannot be recovered by subsequent operations.</strong></p>
<h2 id="normalization_layers_and_homogeneity"><a href="#normalization_layers_and_homogeneity" class="header-anchor">Normalization Layers and Homogeneity</a></h2>
<h3 id="all_normalization_layers_are_0-homogeneous"><a href="#all_normalization_layers_are_0-homogeneous" class="header-anchor">All Normalization Layers Are 0-Homogeneous</a></h3>
<p>The core normalization operation in BatchNorm, LayerNorm, GroupNorm, and InstanceNorm is:</p>
\[\text{normalize}(x) = \frac{x - \mu}{\sigma}\]
<p>where \(\mu\) and \(\sigma\) are computed over different dimensions depending on the normalization type.</p>
<p><strong>This is always 0-homogeneous:</strong></p>
\[\text{normalize}(\alpha x) = \frac{\alpha x - \alpha \mu}{\alpha \sigma} = \frac{\alpha(x - \mu)}{\alpha \sigma} = \frac{x - \mu}{\sigma} = \text{normalize}(x)\]
<p>The key insight: <strong>both the mean and standard deviation scale linearly with \(\alpha\)</strong>, so they cancel out in the division&#33;</p>
<h3 id="comparison_of_normalization_types"><a href="#comparison_of_normalization_types" class="header-anchor">Comparison of Normalization Types</a></h3>
<table><tr><th align="right">Type</th><th align="right">Statistics computed over</th><th align="right">0-homogeneous?</th></tr><tr><td align="right"><strong>BatchNorm</strong></td><td align="right">Batch &#43; spatial dimensions &#40;across samples&#41;</td><td align="right">‚úÖ Yes</td></tr><tr><td align="right"><strong>LayerNorm</strong></td><td align="right">Feature dimensions &#40;per sample&#41;</td><td align="right">‚úÖ Yes</td></tr><tr><td align="right"><strong>GroupNorm</strong></td><td align="right">Groups of channels &#40;per sample&#41;</td><td align="right">‚úÖ Yes</td></tr><tr><td align="right"><strong>InstanceNorm</strong></td><td align="right">Spatial dimensions per channel &#40;per sample&#41;</td><td align="right">‚úÖ Yes</td></tr></table>
<p><strong>All of them break 1-homogeneity equally&#33;</strong> The difference is only in:</p>
<ul>
<li><p>Which dimensions they normalize over</p>
</li>
<li><p>Whether they use batch statistics &#40;BatchNorm&#41; or per-sample statistics &#40;others&#41;</p>
</li>
<li><p>Training vs inference behavior</p>
</li>
</ul>
<h3 id="mathematical_proof_applies_to_all"><a href="#mathematical_proof_applies_to_all" class="header-anchor">Mathematical Proof &#40;applies to all&#41;</a></h3>
<p>For any normalization that computes \(\mu\) and \(\sigma\) from the input:</p>
\[\begin{align}
\mu(\alpha x) &= \alpha \mu(x) \\
\sigma(\alpha x) &= \alpha \sigma(x) \\
\text{normalize}(\alpha x) &= \frac{\alpha x - \alpha \mu(x)}{\alpha \sigma(x)} = \frac{x - \mu(x)}{\sigma(x)} = \text{normalize}(x)
\end{align}\]
<p>‚ùå <strong>All normalization layers are 0-homogeneous</strong></p>
<h3 id="network_with_any_normalization"><a href="#network_with_any_normalization" class="header-anchor">Network with Any Normalization</a></h3>
<p>For network \(N(x) = W_n \circ \text{Norm} \circ W_{n-1} \circ \cdots \circ W_1(x)\):</p>
\[N(\alpha x) = W_n(\text{Norm}(W_{n-1}(\cdots \alpha x))) = W_n(\text{Norm}(\text{something}))\]
<p>Since normalization is 0-homogeneous, regardless of what comes before:</p>
\[N(\alpha x) = \text{does not scale with } \alpha\]
<p>‚ùå <strong>The entire network becomes 0-homogeneous</strong></p>
<h3 id="why_this_matters"><a href="#why_this_matters" class="header-anchor">Why This Matters</a></h3>
<p>While LayerNorm and GroupNorm have advantages over BatchNorm &#40;no batch dependency, better for small batches, etc.&#41;, <strong>they all equally destroy 1-homogeneity</strong>. You cannot fix the homogeneity problem by switching normalization types - you must either:</p>
<ol>
<li><p>Remove normalization entirely</p>
</li>
<li><p>Use Weight Normalization instead</p>
</li>
<li><p>Use NFNet-style architectures</p>
</li>
</ol>
<h2 id="skip_connections_residual_connections"><a href="#skip_connections_residual_connections" class="header-anchor">Skip Connections &#40;Residual Connections&#41;</a></h2>
<h3 id="the_addition_problem"><a href="#the_addition_problem" class="header-anchor">The Addition Problem</a></h3>
<p>Addition is <strong>NOT a homogeneous operation</strong> in general:</p>
\[(f + g)(\alpha x) = f(\alpha x) + g(\alpha x)\]
<p>This only equals \(\alpha(f(x) + g(x))\) if <strong>both</strong> \(f\) and \(g\) have the same homogeneity.</p>
<h3 id="residual_block_with_batchnorm"><a href="#residual_block_with_batchnorm" class="header-anchor">Residual Block with BatchNorm</a></h3>
<p>Consider a typical ResNet block:</p>
\[y = x + F(x)\]
<p>where \(F(x) = W_2 \circ \text{BN} \circ \text{ReLU} \circ W_1(x)\)</p>
<p><strong>What happens with scaling?</strong></p>
<ul>
<li><p><strong>Skip path</strong>: \(x \to \alpha x\) &#40;1-homogeneous, just identity&#41;</p>
</li>
<li><p><strong>Residual path</strong>: \(F(\alpha x) = F(x)\) &#40;0-homogeneous due to BN&#41;</p>
</li>
</ul>
\[y(\alpha x) = \alpha x + F(\alpha x) = \alpha x + F(x)\]
<p>But for 1-homogeneity we need:</p>
\[y(\alpha x) = \alpha(x + F(x))\]
<p><strong>These are NOT equal&#33;</strong> ‚ùå</p>
<h3 id="the_mismatch"><a href="#the_mismatch" class="header-anchor">The Mismatch</a></h3>
\[\begin{align}
\text{Actual:} \quad & \alpha x + F(x) \\
\text{Needed:} \quad & \alpha x + \alpha F(x)
\end{align}\]
<p>The skip connection scales with \(\alpha\), but the BatchNorm path doesn&#39;t. This creates an <strong>imbalance</strong> that breaks homogeneity.</p>
<h3 id="visual_example"><a href="#visual_example" class="header-anchor">Visual Example</a></h3>
<p>With \(\alpha = 2\):</p>
<ul>
<li><p>Skip: \(2x\) &#40;doubled&#41;</p>
</li>
<li><p>BN branch: \(F(x)\) &#40;unchanged&#41;</p>
</li>
<li><p>Sum: \(2x + F(x)\) &#40;not proportional to \(x + F(x)\)&#41;</p>
</li>
</ul>
<p><strong>The relative contribution of each path changes with input scale&#33;</strong></p>
<h2 id="tricks_to_restore_1-homogeneity"><a href="#tricks_to_restore_1-homogeneity" class="header-anchor">Tricks to Restore 1-Homogeneity</a></h2>
<h3 id="why_standard_batchnorm_fails"><a href="#why_standard_batchnorm_fails" class="header-anchor">Why Standard BatchNorm Fails</a></h3>
<p>Standard BatchNorm with affine parameters:</p>
\[\text{BN}(x) = \gamma \frac{x - \mu}{\sigma} + \beta\]
<p>Two problems:</p>
<ol>
<li><p>The normalization \(\frac{x - \mu}{\sigma}\) is <strong>0-homogeneous</strong> &#40;absorbs scaling&#41;</p>
</li>
<li><p>The affine transform &#40;\(\gamma\), \(\beta\)&#41; makes it not even homogeneous at all</p>
</li>
</ol>
<p>Removing \(\gamma\) and \(\beta\) doesn&#39;t help - you still have the 0-homogeneous normalization.</p>
<h3 id="the_impossible_question_can_we_make_batchnorm_1-homogeneous"><a href="#the_impossible_question_can_we_make_batchnorm_1-homogeneous" class="header-anchor">The &quot;Impossible&quot; Question: Can We Make BatchNorm 1-Homogeneous?</a></h3>
<p><strong>Short answer: Not while keeping standard BatchNorm behavior.</strong></p>
<p><strong>Why it&#39;s fundamentally challenging:</strong></p>
<p>The entire <em>point</em> of BatchNorm is to normalize inputs to have standardized statistics:</p>
\[\text{BN}(x) = \frac{x - \mu}{\sigma}\]
<p>This operation <strong>must</strong> be scale-invariant &#40;0-homogeneous&#41; when \(\mu\) and \(\sigma\) are computed from the input being normalized.</p>
<h3 id="the_actual_trick_decouple_statistics_from_the_normalized_input"><a href="#the_actual_trick_decouple_statistics_from_the_normalized_input" class="header-anchor">The ACTUAL Trick: Decouple Statistics from the Normalized Input&#33; üéØ</a></h3>
<p><strong>Key insight:</strong> What if we compute \(\mu\) and \(\sigma\) from a <strong>different source</strong> than the input we&#39;re normalizing?</p>
<h4 id="method_1_ghost_batchnorm"><a href="#method_1_ghost_batchnorm" class="header-anchor">Method 1: Ghost BatchNorm</a></h4>
<p>Compute statistics from a <strong>reference batch</strong>:</p>
<ol>
<li><p><strong>Reference batch</strong>: Get \(\mu_{\text{ref}}, \sigma_{\text{ref}}\) from batch \(B_{\text{ref}}\)</p>
</li>
<li><p><strong>Normalize current batch</strong> \(B\) using those statistics:</p>
</li>
</ol>
\[y = \frac{x - \mu_{\text{ref}}}{\sigma_{\text{ref}}}\]
<p><strong>Problem:</strong> The mean centering still breaks homogeneity&#33;</p>
<p><strong>Solution:</strong> Remove mean centering:</p>
\[y = \frac{x}{\sigma_{\text{ref}}}\]
<p><strong>Is this 1-homogeneous?</strong></p>
\[\frac{\alpha x}{\sigma_{\text{ref}}} = \alpha \frac{x}{\sigma_{\text{ref}}}\]
<p>‚úÖ <strong>YES&#33;</strong> Because \(\sigma_{\text{ref}}\) is independent of the current input \(x\).</p>
<h4 id="method_2_fixed_statistics_inference-mode_trick"><a href="#method_2_fixed_statistics_inference-mode_trick" class="header-anchor">Method 2: Fixed Statistics &#40;Inference-Mode Trick&#41;</a></h4>
<p>In BatchNorm <strong>inference mode</strong>, statistics are <strong>fixed</strong> &#40;running averages from training&#41;:</p>
\[\text{BN}_{\text{inference}}(x) = \gamma \frac{x - \mu_{\text{running}}}{\sigma_{\text{running}}} + \beta\]
<p><strong>Modified version</strong> &#40;no mean centering, no bias&#41;:</p>
\[\text{BN}_{\text{modified}}(x) = \gamma \frac{x}{\sigma_{\text{running}}} = \frac{\gamma}{\sigma_{\text{running}}} x\]
<p>‚úÖ <strong>This is 1-homogeneous&#33;</strong> It&#39;s just multiplication by a constant.</p>
<h4 id="method_3_rms_normalization_with_fixed_scale"><a href="#method_3_rms_normalization_with_fixed_scale" class="header-anchor">Method 3: RMS Normalization with Fixed Scale</a></h4>
\[\text{FixedRMSNorm}(x) = \frac{x}{\sigma_{\text{fixed}}} \cdot g\]
<p>where \(\sigma_{\text{fixed}}\) is from dataset statistics or running average, and \(g\) is learnable.</p>
<p>‚úÖ <strong>1-homogeneous&#33;</strong> Because the denominator doesn&#39;t depend on \(x\).</p>
<h3 id="summary_the_decoupling_trick"><a href="#summary_the_decoupling_trick" class="header-anchor">Summary: The Decoupling Trick</a></h3>
<table><tr><th align="right">Method</th><th align="right">Statistics from</th><th align="right">Normalized input</th><th align="right">1-homo?</th></tr><tr><td align="right">Standard BatchNorm</td><td align="right">Current batch</td><td align="right">Same batch</td><td align="right">‚ùå No</td></tr><tr><td align="right">Ghost BatchNorm &#40;no mean&#41;</td><td align="right">Reference batch</td><td align="right">Different batch</td><td align="right">‚úÖ Yes&#33;</td></tr><tr><td align="right">Inference BN &#40;no mean&#41;</td><td align="right">Running average</td><td align="right">Current input</td><td align="right">‚úÖ Yes&#33;</td></tr><tr><td align="right">Fixed RMSNorm</td><td align="right">Pre-computed</td><td align="right">Current input</td><td align="right">‚úÖ Yes&#33;</td></tr></table>
<p><strong>The key:</strong> When \(\sigma\) doesn&#39;t depend on the input \(x\) being normalized, then \(x/\sigma\) is 1-homogeneous&#33;</p>
<p><strong>The trade-off:</strong> You lose the adaptive, input-dependent normalization that makes BatchNorm effective for training stability. These approaches are closer to having a learned constant scaling factor &#40;like Weight Normalization&#41;.</p>
<h3 id="solution_1_remove_normalization_entirely"><a href="#solution_1_remove_normalization_entirely" class="header-anchor">Solution 1: Remove Normalization Entirely ‚úÖ</a></h3>
<p>Use only 1-homogeneous operations:</p>
\[y = x + W_2(\text{ReLU}(W_1(x)))\]
<p><strong>Why it works:</strong></p>
<ul>
<li><p>Linear layers: 1-homogeneous</p>
</li>
<li><p>ReLU: 1-homogeneous  </p>
</li>
<li><p>Composition: 1-homogeneous</p>
</li>
<li><p>Addition of 1-homogeneous functions: 1-homogeneous</p>
</li>
</ul>
<p><strong>Downside:</strong> Training instability without normalization</p>
<h3 id="solution_2_weight_normalization"><a href="#solution_2_weight_normalization" class="header-anchor">Solution 2: Weight Normalization ‚úÖ</a></h3>
<p><strong>Key idea:</strong> Normalize the <em>weights</em>, not the <em>activations</em>&#33;</p>
\[W_{\text{norm}} = \frac{g}{\|w\|} w\]
<p>Then: \(f(x) = W_{\text{norm}} \cdot x\)</p>
<p><strong>Why it works:</strong></p>
\[f(\alpha x) = \frac{g}{\|w\|} (w \cdot \alpha x) = \alpha \frac{g}{\|w\|} (w \cdot x) = \alpha f(x)\]
<p>‚úÖ <strong>1-homogeneous in \(x\)</strong> because we only normalized the parameters, not the activations&#33;</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><p>Maintains 1-homogeneity</p>
</li>
<li><p>Provides training stability</p>
</li>
<li><p>Works well in practice</p>
</li>
</ul>
<h3 id="solution_3_nfnet_normalizer-free_networks"><a href="#solution_3_nfnet_normalizer-free_networks" class="header-anchor">Solution 3: NFNet &#40;Normalizer-Free Networks&#41; ‚úÖ</a></h3>
<p><strong>Key idea:</strong> Carefully scaled skip connections without any normalization</p>
\[y = x + \alpha \cdot F(x)\]
<p>where \(\alpha\) is a fixed scalar &#40;e.g., \(\alpha = 1/\sqrt{N}\) for \(N\) layers&#41;.</p>
<p><strong>Why it works:</strong> If \(F\) is 1-homogeneous:</p>
\[y(\beta x) = \beta x + \alpha F(\beta x) = \beta x + \alpha \beta F(x) = \beta(x + \alpha F(x)) = \beta \, y(x)\]
<p>‚úÖ <strong>1-homogeneous&#33;</strong></p>
<p><strong>Advantages:</strong></p>
<ul>
<li><p>No normalization layers needed</p>
</li>
<li><p>State-of-the-art results on ImageNet</p>
</li>
<li><p>Faster training &#40;no batch statistics&#41;</p>
</li>
</ul>
<h3 id="solution_4_use_only_1-homogeneous_activations"><a href="#solution_4_use_only_1-homogeneous_activations" class="header-anchor">Solution 4: Use Only 1-Homogeneous Activations ‚úÖ</a></h3>
<ul>
<li><p>‚úÖ ReLU: \(\text{ReLU}(\alpha x) = \alpha \cdot \text{ReLU}(x)\) for \(\alpha > 0\)</p>
</li>
<li><p>‚úÖ Leaky ReLU: Also 1-homogeneous</p>
</li>
<li><p>‚ùå Sigmoid, Tanh: Not homogeneous</p>
</li>
<li><p>‚úÖ Any piecewise linear function through origin: 1-homogeneous</p>
</li>
</ul>
<h3 id="solution_5_remove_bias_terms"><a href="#solution_5_remove_bias_terms" class="header-anchor">Solution 5: Remove Bias Terms&#33; ‚úÖ</a></h3>
<p><strong>The bias problem:</strong></p>
<p>A linear layer with bias: \(f(x) = Wx + b\)</p>
<p><strong>Is this 1-homogeneous?</strong> \(f(\alpha x) = W(\alpha x) + b = \alpha Wx + b \neq \alpha(Wx + b)\)</p>
<p>‚ùå <strong>No&#33;</strong> The bias term \(b\) doesn&#39;t scale.</p>
<p><strong>Why bias breaks homogeneity:</strong></p>
<ul>
<li><p>The term \(Wx\) is 1-homogeneous: \(W(\alpha x) = \alpha Wx\)</p>
</li>
<li><p>But \(b\) is just a constant offset - it doesn&#39;t scale at all</p>
</li>
<li><p>So \(f(\alpha x) = \alpha Wx + b\) while we need \(\alpha f(x) = \alpha Wx + \alpha b\)</p>
</li>
</ul>
<p><strong>The fix: Remove all bias terms&#33;</strong></p>
<p>Use: \(f(x) = Wx\)</p>
<p><strong>Now it&#39;s 1-homogeneous:</strong> \(f(\alpha x) = W(\alpha x) = \alpha Wx = \alpha f(x)\) ‚úÖ</p>
<p><strong>Practical implications:</strong></p>
<p>In PyTorch/TensorFlow, set <code>bias&#61;False</code>:</p>
<pre><code class="language-python"># PyTorch
nn.Linear&#40;in_features, out_features, bias&#61;False&#41;
nn.Conv2d&#40;in_channels, out_channels, kernel_size, bias&#61;False&#41;

# TensorFlow/Keras
Dense&#40;units, use_bias&#61;False&#41;
Conv2D&#40;filters, kernel_size, use_bias&#61;False&#41;</code></pre>
<p><strong>What about the expressiveness loss?</strong></p>
<p>You might think: &quot;But bias terms are important for expressiveness&#33;&quot;</p>
<p><strong>Counter-argument:</strong></p>
<ol>
<li><p>If you&#39;re using normalization &#40;even the decoupled kind&#41;, it often includes learnable scale/shift parameters that can compensate</p>
</li>
<li><p>For 1-homogeneous networks, the bias would break the property anyway</p>
</li>
<li><p>Many successful architectures &#40;like NFNet&#41; work fine without biases in most layers</p>
</li>
<li><p>The skip connections in residual networks can provide the &quot;offset&quot; functionality</p>
</li>
</ol>
<p><strong>Exception:</strong> You can have bias in the <strong>final output layer</strong> if you don&#39;t need end-to-end 1-homogeneity &#40;e.g., for classification, you often don&#39;t care if the final logits are 1-homogeneous&#41;.</p>
<h3 id="summary_building_a_fully_1-homogeneous_network"><a href="#summary_building_a_fully_1-homogeneous_network" class="header-anchor">Summary: Building a Fully 1-Homogeneous Network</a></h3>
<p>To make a network 1-homogeneous, you must:</p>
<ol>
<li><p>‚úÖ Use linear layers <strong>without bias</strong>: \(f(x) = Wx\)</p>
</li>
<li><p>‚úÖ Use 1-homogeneous activations: ReLU, Leaky ReLU</p>
</li>
<li><p>‚úÖ Either:</p>
<ul>
<li><p>Remove normalization entirely, OR</p>
</li>
<li><p>Use Weight Normalization, OR</p>
</li>
<li><p>Use decoupled normalization &#40;statistics from different source&#41;, OR</p>
</li>
<li><p>Use NFNet-style scaled skip connections</p>
</li>
</ul>
</li>
<li><p>‚úÖ Ensure skip connections maintain homogeneity: both branches must be 1-homogeneous</p>
</li>
<li><p>‚ùå <strong>No bias terms anywhere</strong> &#40;except possibly final layer&#41;</p>
</li>
<li><p>‚ùå <strong>No 0-homogeneous operations</strong> &#40;no standard BatchNorm/LayerNorm with coupled statistics&#41;</p>
</li>
</ol>
<p><strong>Example 1-homogeneous residual block:</strong> \(y = x + \alpha \cdot W_2(\text{ReLU}(W_1(x)))\)</p>
<p>where \(W_1, W_2\) have no bias, and \(\alpha\) is a fixed scalar.</p>
<h3 id="practical_recommendations"><a href="#practical_recommendations" class="header-anchor">Practical Recommendations</a></h3>
<table><tr><th align="right">Method</th><th align="right">1-Homogeneous?</th><th align="right">Training Stability</th><th align="right">Practical?</th></tr><tr><td align="right">Remove all normalization</td><td align="right">‚úÖ</td><td align="right">‚ö†Ô∏è Poor</td><td align="right">‚ö†Ô∏è Challenging</td></tr><tr><td align="right">Weight Normalization</td><td align="right">‚úÖ</td><td align="right">‚úÖ Good</td><td align="right">‚úÖ Yes</td></tr><tr><td align="right">NFNet &#40;scaled residuals&#41;</td><td align="right">‚úÖ</td><td align="right">‚úÖ Good</td><td align="right">‚úÖ Yes &#40;SOTA&#41;</td></tr><tr><td align="right">Ghost BN &#40;no mean&#41;</td><td align="right">‚úÖ</td><td align="right">‚ö†Ô∏è Moderate</td><td align="right">‚ö†Ô∏è Limited use</td></tr><tr><td align="right">Inference BN &#40;no mean&#41;</td><td align="right">‚úÖ</td><td align="right">‚ùå Only at inference</td><td align="right">‚ùå No</td></tr><tr><td align="right">Standard BatchNorm</td><td align="right">‚ùå</td><td align="right">‚úÖ Good</td><td align="right">‚úÖ But not 1-homo</td></tr><tr><td align="right">LayerNorm</td><td align="right">‚ùå</td><td align="right">‚úÖ Good</td><td align="right">‚úÖ But not 1-homo</td></tr></table>
<p><strong>Best practical approaches:</strong> Use <strong>Weight Normalization</strong> or <strong>NFNet-style</strong> architectures for 1-homogeneous networks with good training stability.</p>
<h2 id="conclusion"><a href="#conclusion" class="header-anchor">Conclusion</a></h2>
<p><strong>To maintain 1-homogeneity through a composition:</strong></p>
<ul>
<li><p>‚úÖ All functions must be 1-homogeneous</p>
</li>
<li><p>‚ùå Even ONE 0-homogeneous function breaks the chain</p>
</li>
<li><p>‚ùå Skip connections mixing different homogeneities break it too</p>
</li>
<li><p>The 0-homogeneous function &quot;absorbs&quot; all scaling information</p>
</li>
<li><p>Addition requires <strong>matching homogeneity</strong> on both branches</p>
</li>
<li><p>Subsequent layers cannot recover what was lost</p>
</li>
<li><p><strong>Trick exists:</strong> Decouple normalization statistics from the input being normalized</p>
</li>
<li><p><strong>Practical solutions:</strong> Weight Normalization and NFNet architectures</p>
</li>
</ul>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: November 19, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
